{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import data_preparation as prep, data_process as proc\n",
    "from utils.utils_baselines import *\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from julia import Julia\n",
    "Julia(sysimage='/home/gridsan/groups/IAI/images/2.0.0/julia-1.4.2/sys.so', compiled_modules = False)\n",
    "from interpretableai import iai\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baselines_train(x_df, x, test_size = 0.2):\n",
    "    y_baseline_speed = np.array(x_df['speed_forecast'])\n",
    "    y_baseline_cos_wind = np.array(x_df['cos_wind_dir_forecast'])\n",
    "    y_baseline_sin_wind = np.array(x_df['sin_wind_dir_forecast'])\n",
    "    _, _, y_train_baseline_speed, y_test_baseline_speed = train_test_split(x, y_baseline_speed, test_size = test_size, shuffle = False)\n",
    "    _, _, y_train_baseline_cos_wind, y_test_baseline_cos_wind = train_test_split(x, y_baseline_cos_wind, test_size = test_size, shuffle = False)\n",
    "    _, _, y_train_baseline_sin_wind, y_test_baseline_sin_wind = train_test_split(x, y_baseline_sin_wind, test_size = test_size, shuffle = False)\n",
    "    y_baseline_dangerous_scenarios = get_all_dangerous_scenarios(y_train_baseline_speed, y_train_baseline_cos_wind, y_train_baseline_sin_wind)\n",
    "    y_baseline_scenarios = get_all_scenarios(y_train_baseline_speed, y_train_baseline_cos_wind, y_train_baseline_sin_wind, b_scenarios=True)\n",
    "    return y_train_baseline_speed, y_train_baseline_cos_wind, y_train_baseline_sin_wind, y_baseline_dangerous_scenarios, y_baseline_scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read csv semester csv files from 2015s2 to 2020s1\n",
      "smooth wind direction\n",
      "generate seasonality categorical feature\n",
      "reading forecast data\n",
      "smooth wind direction\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "\n",
    "# call data_preparation.py\n",
    "if True:\n",
    "    measurement = prep.prepare_measurement()\n",
    "    forecast = prep.prepare_forecast()\n",
    "\n",
    "    # keep useful columns\n",
    "    measurement = measurement[['speed', 'cos_wind_dir', 'sin_wind_dir', 'temp', 'radiation', 'precip', 'season']]\n",
    "\n",
    "    # call data_process.py\n",
    "\n",
    "    steps_in = 48\n",
    "    steps_out = 1\n",
    "\n",
    "    x_df, y_df, x, y_speed = proc.prepare_x_y(measurement, forecast, steps_in, steps_out, 'speed')\n",
    "    _, _, _, y_cos = proc.prepare_x_y(measurement, forecast, steps_in, steps_out, 'cos_wind_dir')\n",
    "    _, _, _, y_sin = proc.prepare_x_y(measurement, forecast, steps_in, steps_out, 'sin_wind_dir')\n",
    "    y_scenarios = get_all_scenarios(np.array(y_speed), np.array(y_cos), np.array(y_sin), b_scenarios=True)\n",
    "    y_dangerous = get_all_dangerous_scenarios(np.array(y_speed), np.array(y_cos), np.array(y_sin))\n",
    "    X_train, X_test, y_train_dangerous, y_test_dangerous = train_test_split(x, y_dangerous, test_size=0.2,\n",
    "                                                                            shuffle=False)\n",
    "    _, _, y_train_scenarios, y_test_scenarios = train_test_split(x, y_scenarios, test_size=0.2, shuffle=False)\n",
    "    #_, _, y_train_speed, y_test_speed = train_test_split(x, y_speed, test_size=0.2, shuffle=False)\n",
    "    #_, _, y_train_cos, y_test_cos = train_test_split(x, y_cos, test_size=0.2, shuffle=False)\n",
    "    #_, _, y_train_sin, y_test_sin = train_test_split(x, y_sin, test_size=0.2, shuffle=False)\n",
    "    #\n",
    "    #names = list(x_df.columns)\n",
    "    #names.remove('present_time')\n",
    "    #names.remove('forecast_time')\n",
    "    #X_train2 = pd.DataFrame(X_train)\n",
    "    #X_train2.columns = names\n",
    "    #(X_train_reg, y_train_speed_reg), _ = iai.split_data('regression', X_train2, np.array(y_train_speed), train_proportion=0.9999)\n",
    "    #(_, y_train_cos_reg), _ = iai.split_data('regression', X_train2, np.array(y_train_cos), train_proportion=0.9999)\n",
    "    #(_, y_train_sin_reg), _ = iai.split_data('regression', X_train2, np.array(y_train_sin), train_proportion=0.9999)\n",
    "    ###BASELINES\n",
    "    y_test_baseline_speed, y_test_baseline_cos_wind, y_test_baseline_sin_wind, y_test_baseline_dangerous_scenarios, y_test_baseline_scenarios = get_baselines(\n",
    "        x_df, x)\n",
    "    y_train_baseline_speed, y_train_baseline_cos_wind, y_train_baseline_sin_wind, y_train_baseline_dangerous_scenarios, y_train_baseline_scenarios = get_baselines_train(\n",
    "        x_df, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_cos = '/home/gridsan/leobix/safi/Trees3/vfinal_Regression_tree_cos_in48_out1_sparsity_0.json'\n",
    "path_sin = '/home/gridsan/leobix/safi/Trees3/vfinal_Regression_tree_sin_in48_out1_sparsity_0.json'\n",
    "path_speed = '/home/gridsan/leobix/safi/Trees3/vfinal_Regression_tree_speed_in48_out1_sparsity_0.json'\n",
    "path_class = '/home/gridsan/leobix/safi/Trees/vfinal_Classification_tree_dangerous_in48_out1_criterion_gini.json'\n",
    "\n",
    "lnr_cos = iai.read_json(path_cos)\n",
    "lnr_sin = iai.read_json(path_sin)\n",
    "lnr_speed = iai.read_json(path_speed)\n",
    "lnr_dangerous = iai.read_json(path_class)\n",
    "\n",
    "y_hat_speed = lnr_speed.predict(X_test)\n",
    "y_hat_cos = lnr_cos.predict(X_test)\n",
    "y_hat_sin = lnr_sin.predict(X_test)\n",
    "y_hat_dangerous_from_regression = get_all_dangerous_scenarios(y_hat_speed, y_hat_cos, y_hat_sin)\n",
    "y_hat_dangerous_from_classification_proba = np.array(lnr_dangerous.predict_proba(X_test)['1'])\n",
    "y_hat_dangerous_from_classification = np.array(lnr_dangerous.predict(X_test))\n",
    "\n",
    "y_hat_speed_train = lnr_speed.predict(X_train)\n",
    "y_hat_cos_train = lnr_cos.predict(X_train)\n",
    "y_hat_sin_train = lnr_sin.predict(X_train)\n",
    "y_hat_dangerous_from_regression_train = get_all_dangerous_scenarios(y_hat_speed_train, y_hat_cos_train, y_hat_sin_train)\n",
    "y_hat_dangerous_from_classification_proba_train = np.array(lnr_dangerous.predict_proba(X_train)['1'])\n",
    "y_hat_dangerous_from_classification_train = np.array(lnr_dangerous.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ensemble = pd.read_csv('results_ensemble/xgboost_result_train_1.csv?dl=0')\n",
    "X_test_ensemble = pd.read_csv('results_ensemble/xgboost_result_test_1.csv?dl=0')\n",
    "X_train_ensemble = X_train_ensemble.drop(['true', 'baseline'], axis = 1)\n",
    "#X_train_ensemble = X_train_ensemble.drop([0], axis = 0)\n",
    "X_test_ensemble = X_test_ensemble.drop(['true', 'baseline'], axis = 1)\n",
    "X_test_ensemble_dt = pd.read_csv('results_ensemble/dt_result_test_1.csv?dl=0') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ensemble['OCT_speed'] = y_hat_speed\n",
    "X_test_ensemble['OCT_cos'] = y_hat_cos\n",
    "X_test_ensemble['OCT_sin'] = y_hat_sin\n",
    "X_test_ensemble['OCT_indirect'] = y_hat_dangerous_from_regression\n",
    "X_test_ensemble['OCT_direct'] = y_hat_dangerous_from_classification\n",
    "X_test_ensemble['OCT_direct_probas'] = y_hat_dangerous_from_classification_proba\n",
    "X_test_ensemble['NUMTECH_speed'] = y_test_baseline_speed\n",
    "X_test_ensemble['NUMTECH_cos'] = y_test_baseline_cos_wind\n",
    "X_test_ensemble['NUMTECH_sin'] = y_test_baseline_sin_wind\n",
    "X_test_ensemble['NUMTECH_indirect'] = y_test_baseline_dangerous_scenarios\n",
    "\n",
    "X_test_ensemble['DT_indirect'] = X_test_ensemble_dt['dangerous_indirect']\n",
    "X_test_ensemble['DT_direct'] = X_test_ensemble_dt['dangerous']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ensemble['OCT_speed'] = y_hat_speed_train\n",
    "X_train_ensemble['OCT_cos'] = y_hat_cos_train\n",
    "X_train_ensemble['OCT_sin'] = y_hat_sin_train\n",
    "X_train_ensemble['OCT_indirect'] = y_hat_dangerous_from_regression_train\n",
    "X_train_ensemble['OCT_direct'] = y_hat_dangerous_from_classification_train\n",
    "X_train_ensemble['OCT_direct_probas'] = y_hat_dangerous_from_classification_proba_train\n",
    "X_train_ensemble['NUMTECH_speed'] = y_train_baseline_speed\n",
    "X_train_ensemble['NUMTECH_cos'] = y_train_baseline_cos_wind\n",
    "X_train_ensemble['NUMTECH_sin'] = y_train_baseline_sin_wind\n",
    "X_train_ensemble['NUMTECH_indirect'] = y_train_baseline_dangerous_scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Majority vote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using all models available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7992  284]\n",
      " [  98  120]]\n",
      "0.6812555085380129\n",
      "Total cost is  862\n"
     ]
    }
   ],
   "source": [
    "X_test_majority_vote = X_test_ensemble[['NUMTECH_indirect', 'OCT_indirect', 'OCT_direct', 'dangerous_indirect', 'dangerous']]\n",
    "consensus_majority = np.array(X_test_majority_vote.sum(axis = 1) >=2)*1\n",
    "c = confusion_matrix(y_test_dangerous,consensus_majority)\n",
    "print(c)\n",
    "print(f1_score(y_test_dangerous, consensus_majority, average = 'macro'))\n",
    "print('Total cost is ', c[0,1]*2+c[1,0]*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8093  183]\n",
      " [ 133   85]]\n",
      "0.6653225223230206\n",
      "Total cost is  765\n"
     ]
    }
   ],
   "source": [
    "X_test_majority_vote = X_test_ensemble[['NUMTECH_indirect', 'OCT_indirect', 'OCT_direct', 'dangerous_indirect', 'dangerous']]\n",
    "consensus_majority = np.array(X_test_majority_vote.sum(axis = 1) >=3)*1\n",
    "c = confusion_matrix(y_test_dangerous,consensus_majority)\n",
    "print(c)\n",
    "print(f1_score(y_test_dangerous, consensus_majority, average = 'macro'))\n",
    "print('Total cost is ', c[0,1]*2+c[1,0]*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8213   63]\n",
      " [ 157   61]]\n",
      "0.6717543789386559\n",
      "Total cost is  597\n"
     ]
    }
   ],
   "source": [
    "X_test_majority_vote = X_test_ensemble[['NUMTECH_indirect', 'OCT_indirect', 'OCT_direct', 'dangerous_indirect', 'dangerous']]\n",
    "consensus_majority = np.array(X_test_majority_vote.sum(axis = 1) >=4)*1\n",
    "c = confusion_matrix(y_test_dangerous,consensus_majority)\n",
    "print(c)\n",
    "print(f1_score(y_test_dangerous, consensus_majority, average = 'macro'))\n",
    "print('Total cost is ', c[0,1]*2+c[1,0]*3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using only best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8209   67]\n",
      " [ 148   70]]\n",
      "0.6907200432196982\n",
      "Total cost is  578\n"
     ]
    }
   ],
   "source": [
    "X_test_majority_vote = X_test_ensemble[['NUMTECH_indirect', 'OCT_direct', 'dangerous_indirect', 'dangerous']]\n",
    "consensus_majority = np.array(X_test_majority_vote.sum(axis = 1) >=3)*1\n",
    "c = confusion_matrix(y_test_dangerous,consensus_majority)\n",
    "print(c)\n",
    "print(f1_score(y_test_dangerous, consensus_majority, average = 'macro'))\n",
    "print('Total cost is ', c[0,1]*2+c[1,0]*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8024  252]\n",
      " [ 105  113]]\n",
      "0.6829442138661026\n",
      "Total cost is  819\n"
     ]
    }
   ],
   "source": [
    "X_test_majority_vote = X_test_ensemble[['NUMTECH_indirect', 'OCT_direct', 'dangerous_indirect', 'dangerous']]\n",
    "consensus_majority = np.array(X_test_majority_vote.sum(axis = 1) >=2)*1\n",
    "c = confusion_matrix(y_test_dangerous,consensus_majority)\n",
    "print(c)\n",
    "print(f1_score(y_test_dangerous, consensus_majority, average = 'macro'))\n",
    "print('Total cost is ', c[0,1]*2+c[1,0]*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8213   63]\n",
      " [ 152   66]]\n",
      "0.6837417807862147\n",
      "Total cost is  582\n"
     ]
    }
   ],
   "source": [
    "X_test_majority_vote = X_test_ensemble[['NUMTECH_indirect', 'OCT_direct', 'OCT_indirect','dangerous_indirect']]\n",
    "consensus_majority = np.array(X_test_majority_vote.sum(axis = 1) >=3)*1\n",
    "c = confusion_matrix(y_test_dangerous,consensus_majority)\n",
    "print(c)\n",
    "print(f1_score(y_test_dangerous, consensus_majority, average = 'macro'))\n",
    "print('Total cost is ', c[0,1]*2+c[1,0]*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8209   67]\n",
      " [ 148   70]]\n",
      "0.6907200432196982\n",
      "Total cost is  578\n"
     ]
    }
   ],
   "source": [
    "X_test_majority_vote = X_test_ensemble[['NUMTECH_indirect', 'OCT_direct', 'dangerous_indirect', 'dangerous']]\n",
    "consensus_majority = np.array(X_test_majority_vote.sum(axis = 1) >=3)*1\n",
    "c = confusion_matrix(y_test_dangerous,consensus_majority)\n",
    "print(c)\n",
    "print(f1_score(y_test_dangerous, consensus_majority, average = 'macro'))\n",
    "print('Total cost is ', c[0,1]*2+c[1,0]*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8200   76]\n",
      " [ 136   82]]\n",
      "0.7117041769343874\n",
      "Total cost is  560\n"
     ]
    }
   ],
   "source": [
    "X_test_majority_vote = X_test_ensemble[['NUMTECH_indirect', 'OCT_direct', 'dangerous_indirect', 'dangerous_indirect', 'dangerous']]\n",
    "consensus_majority = np.array(X_test_majority_vote.sum(axis = 1) >=3)*1\n",
    "c = confusion_matrix(y_test_dangerous,consensus_majority)\n",
    "print(c)\n",
    "print(f1_score(y_test_dangerous, consensus_majority, average = 'macro'))\n",
    "print('Total cost is ', c[0,1]*2+c[1,0]*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8200   76]\n",
      " [ 138   80]]\n",
      "0.7074633918047339\n",
      "Total cost is  566\n"
     ]
    }
   ],
   "source": [
    "X_test_majority_vote = X_test_ensemble[['NUMTECH_indirect', 'OCT_direct', 'dangerous_indirect', 'dangerous_indirect', 'dangerous']]\n",
    "consensus_majority = np.array(X_test_majority_vote.sum(axis = 1) >=3)*1\n",
    "c = confusion_matrix(y_test_dangerous,consensus_majority)\n",
    "print(c)\n",
    "print(f1_score(y_test_dangerous, consensus_majority, average = 'macro'))\n",
    "print('Total cost is ', c[0,1]*2+c[1,0]*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### Costs are ######\n",
      "NUMTECH:  1057\n",
      "OCT indirect:  829\n",
      "OCT direct:  838\n",
      "XGB indirect:  568\n",
      "XGB direct:  957\n",
      "DT indirect:  810\n",
      "DT direct:  1228\n",
      "Majority vote w/o OCT 561\n",
      "Majority vote w/o OCT 557\n"
     ]
    }
   ],
   "source": [
    "FP = 2\n",
    "FN = 3\n",
    "c = confusion_matrix(y_test_dangerous,X_test_ensemble['NUMTECH_indirect'])\n",
    "#print(f1_score(y_test_dangerous, consensus_majority, average = 'macro'))\n",
    "print('###### Costs are ######')\n",
    "print('NUMTECH: ', c[0,1]*FP+c[1,0]*3)\n",
    "\n",
    "c = confusion_matrix(y_test_dangerous,X_test_ensemble['OCT_indirect'])\n",
    "print('OCT indirect: ', c[0,1]*FP+c[1,0]*3)\n",
    "\n",
    "c = confusion_matrix(y_test_dangerous,X_test_ensemble['OCT_direct'])\n",
    "print('OCT direct: ', c[0,1]*FP+c[1,0]*3)\n",
    "\n",
    "c = confusion_matrix(y_test_dangerous,X_test_ensemble['dangerous_indirect'])\n",
    "print('XGB indirect: ', c[0,1]*FP+c[1,0]*3)\n",
    "\n",
    "c = confusion_matrix(y_test_dangerous,X_test_ensemble['dangerous'])\n",
    "print('XGB direct: ', c[0,1]*FP+c[1,0]*3)\n",
    "\n",
    "c = confusion_matrix(y_test_dangerous,X_test_ensemble['DT_indirect'])\n",
    "print('DT indirect: ', c[0,1]*FP+c[1,0]*3)\n",
    "\n",
    "c = confusion_matrix(y_test_dangerous,X_test_ensemble['DT_direct'])\n",
    "print('DT direct: ', c[0,1]*FP+c[1,0]*3)\n",
    "\n",
    "X_test_majority_vote = X_test_ensemble[['NUMTECH_indirect', 'DT_indirect', 'dangerous_indirect', 'dangerous_indirect', 'dangerous']]\n",
    "consensus_majority = np.array(X_test_majority_vote.sum(axis = 1) >=3)*1\n",
    "c = confusion_matrix(y_test_dangerous,consensus_majority)\n",
    "print('Majority vote w/o OCT', c[0,1]*FP+c[1,0]*3)\n",
    "\n",
    "X_test_majority_vote = X_test_ensemble[['NUMTECH_indirect', 'DT_indirect', 'OCT_indirect', 'dangerous_indirect', 'dangerous_indirect', 'dangerous']]\n",
    "consensus_majority = np.array(X_test_majority_vote.sum(axis = 1) >=4)*1\n",
    "c = confusion_matrix(y_test_dangerous,consensus_majority)\n",
    "print('Majority vote w/o OCT', c[0,1]*FP+c[1,0]*FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read csv semester csv files from 2015s2 to 2020s1\n",
      "smooth wind direction\n",
      "generate seasonality categorical feature\n",
      "reading forecast data\n",
      "smooth wind direction\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "\n",
    "# call data_preparation.py\n",
    "if True:\n",
    "    measurement = prep.prepare_measurement()\n",
    "    forecast = prep.prepare_forecast()\n",
    "\n",
    "    # keep useful columns\n",
    "    measurement = measurement[['speed', 'cos_wind_dir', 'sin_wind_dir', 'temp', 'radiation', 'precip', 'season']]\n",
    "\n",
    "    # call data_process.py\n",
    "\n",
    "    steps_in = 48\n",
    "    steps_out = 2\n",
    "\n",
    "    x_df, y_df, x, y_speed = proc.prepare_x_y(measurement, forecast, steps_in, steps_out, 'speed')\n",
    "    _, _, _, y_cos = proc.prepare_x_y(measurement, forecast, steps_in, steps_out, 'cos_wind_dir')\n",
    "    _, _, _, y_sin = proc.prepare_x_y(measurement, forecast, steps_in, steps_out, 'sin_wind_dir')\n",
    "    y_scenarios = get_all_scenarios(np.array(y_speed), np.array(y_cos), np.array(y_sin), b_scenarios=True)\n",
    "    y_dangerous = get_all_dangerous_scenarios(np.array(y_speed), np.array(y_cos), np.array(y_sin))\n",
    "    X_train, X_test, y_train_dangerous, y_test_dangerous = train_test_split(x, y_dangerous, test_size=0.2,\n",
    "                                                                            shuffle=False)\n",
    "    _, _, y_train_scenarios, y_test_scenarios = train_test_split(x, y_scenarios, test_size=0.2, shuffle=False)\n",
    "    #_, _, y_train_speed, y_test_speed = train_test_split(x, y_speed, test_size=0.2, shuffle=False)\n",
    "    #_, _, y_train_cos, y_test_cos = train_test_split(x, y_cos, test_size=0.2, shuffle=False)\n",
    "    #_, _, y_train_sin, y_test_sin = train_test_split(x, y_sin, test_size=0.2, shuffle=False)\n",
    "    #\n",
    "    #names = list(x_df.columns)\n",
    "    #names.remove('present_time')\n",
    "    #names.remove('forecast_time')\n",
    "    #X_train2 = pd.DataFrame(X_train)\n",
    "    #X_train2.columns = names\n",
    "    #(X_train_reg, y_train_speed_reg), _ = iai.split_data('regression', X_train2, np.array(y_train_speed), train_proportion=0.9999)\n",
    "    #(_, y_train_cos_reg), _ = iai.split_data('regression', X_train2, np.array(y_train_cos), train_proportion=0.9999)\n",
    "    #(_, y_train_sin_reg), _ = iai.split_data('regression', X_train2, np.array(y_train_sin), train_proportion=0.9999)\n",
    "    ###BASELINES\n",
    "    y_test_baseline_speed, y_test_baseline_cos_wind, y_test_baseline_sin_wind, y_test_baseline_dangerous_scenarios, y_test_baseline_scenarios = get_baselines(\n",
    "        x_df, x)\n",
    "    y_train_baseline_speed, y_train_baseline_cos_wind, y_train_baseline_sin_wind, y_train_baseline_dangerous_scenarios, y_train_baseline_scenarios = get_baselines_train(\n",
    "        x_df, x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_cos = '/home/gridsan/leobix/safi/Trees3/vfinal_Regression_tree_cos_in48_out2_sparsity_0.json'\n",
    "path_sin = '/home/gridsan/leobix/safi/Trees3/vfinal_Regression_tree_sin_in48_out2_sparsity_0.json'\n",
    "path_speed = '/home/gridsan/leobix/safi/Trees3/vfinal_Regression_tree_speed_in48_out2_sparsity_0.json'\n",
    "path_class = '/home/gridsan/leobix/safi/Trees/vfinal_Classification_tree_dangerous_in48_out2_criterion_gini.json'\n",
    "\n",
    "lnr_cos = iai.read_json(path_cos)\n",
    "lnr_sin = iai.read_json(path_sin)\n",
    "lnr_speed = iai.read_json(path_speed)\n",
    "lnr_dangerous = iai.read_json(path_class)\n",
    "\n",
    "y_hat_speed = lnr_speed.predict(X_test)\n",
    "y_hat_cos = lnr_cos.predict(X_test)\n",
    "y_hat_sin = lnr_sin.predict(X_test)\n",
    "y_hat_dangerous_from_regression = get_all_dangerous_scenarios(y_hat_speed, y_hat_cos, y_hat_sin)\n",
    "y_hat_dangerous_from_classification_proba = np.array(lnr_dangerous.predict_proba(X_test)['1'])\n",
    "y_hat_dangerous_from_classification = np.array(lnr_dangerous.predict(X_test))\n",
    "\n",
    "y_hat_speed_train = lnr_speed.predict(X_train)\n",
    "y_hat_cos_train = lnr_cos.predict(X_train)\n",
    "y_hat_sin_train = lnr_sin.predict(X_train)\n",
    "y_hat_dangerous_from_regression_train = get_all_dangerous_scenarios(y_hat_speed_train, y_hat_cos_train, y_hat_sin_train)\n",
    "y_hat_dangerous_from_classification_proba_train = np.array(lnr_dangerous.predict_proba(X_train)['1'])\n",
    "y_hat_dangerous_from_classification_train = np.array(lnr_dangerous.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ensemble = pd.read_csv('results_ensemble/xgboost_result_train_2.csv?dl=0')\n",
    "X_test_ensemble = pd.read_csv('results_ensemble/xgboost_result_test_2.csv?dl=0')\n",
    "X_train_ensemble = X_train_ensemble.drop(['true', 'baseline'], axis = 1)\n",
    "#X_train_ensemble = X_train_ensemble.drop([0], axis = 0)\n",
    "X_test_ensemble = X_test_ensemble.drop(['true', 'baseline'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ensemble['OCT_speed'] = y_hat_speed\n",
    "X_test_ensemble['OCT_cos'] = y_hat_cos\n",
    "X_test_ensemble['OCT_sin'] = y_hat_sin\n",
    "X_test_ensemble['OCT_indirect'] = y_hat_dangerous_from_regression\n",
    "X_test_ensemble['OCT_direct'] = y_hat_dangerous_from_classification\n",
    "X_test_ensemble['OCT_direct_probas'] = y_hat_dangerous_from_classification_proba\n",
    "X_test_ensemble['NUMTECH_speed'] = y_test_baseline_speed\n",
    "X_test_ensemble['NUMTECH_cos'] = y_test_baseline_cos_wind\n",
    "X_test_ensemble['NUMTECH_sin'] = y_test_baseline_sin_wind\n",
    "X_test_ensemble['NUMTECH_indirect'] = y_test_baseline_dangerous_scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ensemble['OCT_speed'] = y_hat_speed_train\n",
    "X_train_ensemble['OCT_cos'] = y_hat_cos_train\n",
    "X_train_ensemble['OCT_sin'] = y_hat_sin_train\n",
    "X_train_ensemble['OCT_indirect'] = y_hat_dangerous_from_regression_train\n",
    "X_train_ensemble['OCT_direct'] = y_hat_dangerous_from_classification_train\n",
    "X_train_ensemble['OCT_direct_probas'] = y_hat_dangerous_from_classification_proba_train\n",
    "X_train_ensemble['NUMTECH_speed'] = y_train_baseline_speed\n",
    "X_train_ensemble['NUMTECH_cos'] = y_train_baseline_cos_wind\n",
    "X_train_ensemble['NUMTECH_sin'] = y_train_baseline_sin_wind\n",
    "X_train_ensemble['NUMTECH_indirect'] = y_train_baseline_dangerous_scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### Costs are ######\n",
      "NUMTECH:  1059\n",
      "OCT indirect:  728\n",
      "OCT direct:  914\n",
      "XGB indirect:  614\n",
      "XGB direct:  908\n",
      "Majority vote  601\n"
     ]
    }
   ],
   "source": [
    "c = confusion_matrix(y_test_dangerous,X_test_ensemble['NUMTECH_indirect'])\n",
    "#print(f1_score(y_test_dangerous, consensus_majority, average = 'macro'))\n",
    "print('###### Costs are ######')\n",
    "print('NUMTECH: ', c[0,1]*2+c[1,0]*3)\n",
    "\n",
    "c = confusion_matrix(y_test_dangerous,X_test_ensemble['OCT_indirect'])\n",
    "print('OCT indirect: ', c[0,1]*2+c[1,0]*3)\n",
    "\n",
    "c = confusion_matrix(y_test_dangerous,X_test_ensemble['OCT_direct'])\n",
    "print('OCT direct: ', c[0,1]*2+c[1,0]*3)\n",
    "\n",
    "c = confusion_matrix(y_test_dangerous,X_test_ensemble['dangerous_indirect'])\n",
    "print('XGB indirect: ', c[0,1]*2+c[1,0]*3)\n",
    "\n",
    "c = confusion_matrix(y_test_dangerous,X_test_ensemble['dangerous'])\n",
    "print('XGB direct: ', c[0,1]*2+c[1,0]*3)\n",
    "\n",
    "X_test_majority_vote = X_test_ensemble[['NUMTECH_indirect', 'OCT_direct', 'dangerous_indirect', 'dangerous_indirect', 'dangerous']]\n",
    "consensus_majority = np.array(X_test_majority_vote.sum(axis = 1) >=3)*1\n",
    "c = confusion_matrix(y_test_dangerous,consensus_majority)\n",
    "print('Majority vote ', c[0,1]*2+c[1,0]*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8217   59]\n",
      " [ 161   57]]\n",
      "0.664053662819671\n",
      "Total cost is  601\n"
     ]
    }
   ],
   "source": [
    "X_test_majority_vote = X_test_ensemble[['NUMTECH_indirect', 'OCT_direct', 'dangerous_indirect', 'dangerous_indirect', 'dangerous']]\n",
    "consensus_majority = np.array(X_test_majority_vote.sum(axis = 1) >=3)*1\n",
    "c = confusion_matrix(y_test_dangerous,consensus_majority)\n",
    "print(c)\n",
    "print(f1_score(y_test_dangerous, consensus_majority, average = 'macro'))\n",
    "print('Total cost is ', c[0,1]*2+c[1,0]*3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read csv semester csv files from 2015s2 to 2020s1\n",
      "smooth wind direction\n",
      "generate seasonality categorical feature\n",
      "reading forecast data\n",
      "smooth wind direction\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "\n",
    "# call data_preparation.py\n",
    "if True:\n",
    "    measurement = prep.prepare_measurement()\n",
    "    forecast = prep.prepare_forecast()\n",
    "\n",
    "    # keep useful columns\n",
    "    measurement = measurement[['speed', 'cos_wind_dir', 'sin_wind_dir', 'temp', 'radiation', 'precip', 'season']]\n",
    "\n",
    "    # call data_process.py\n",
    "\n",
    "    steps_in = 48\n",
    "    steps_out = 3\n",
    "\n",
    "    x_df, y_df, x, y_speed = proc.prepare_x_y(measurement, forecast, steps_in, steps_out, 'speed')\n",
    "    _, _, _, y_cos = proc.prepare_x_y(measurement, forecast, steps_in, steps_out, 'cos_wind_dir')\n",
    "    _, _, _, y_sin = proc.prepare_x_y(measurement, forecast, steps_in, steps_out, 'sin_wind_dir')\n",
    "    y_scenarios = get_all_scenarios(np.array(y_speed), np.array(y_cos), np.array(y_sin), b_scenarios=True)\n",
    "    y_dangerous = get_all_dangerous_scenarios(np.array(y_speed), np.array(y_cos), np.array(y_sin))\n",
    "    X_train, X_test, y_train_dangerous, y_test_dangerous = train_test_split(x, y_dangerous, test_size=0.2,\n",
    "                                                                            shuffle=False)\n",
    "    _, _, y_train_scenarios, y_test_scenarios = train_test_split(x, y_scenarios, test_size=0.2, shuffle=False)\n",
    "    #_, _, y_train_speed, y_test_speed = train_test_split(x, y_speed, test_size=0.2, shuffle=False)\n",
    "    #_, _, y_train_cos, y_test_cos = train_test_split(x, y_cos, test_size=0.2, shuffle=False)\n",
    "    #_, _, y_train_sin, y_test_sin = train_test_split(x, y_sin, test_size=0.2, shuffle=False)\n",
    "    #\n",
    "    #names = list(x_df.columns)\n",
    "    #names.remove('present_time')\n",
    "    #names.remove('forecast_time')\n",
    "    #X_train2 = pd.DataFrame(X_train)\n",
    "    #X_train2.columns = names\n",
    "    #(X_train_reg, y_train_speed_reg), _ = iai.split_data('regression', X_train2, np.array(y_train_speed), train_proportion=0.9999)\n",
    "    #(_, y_train_cos_reg), _ = iai.split_data('regression', X_train2, np.array(y_train_cos), train_proportion=0.9999)\n",
    "    #(_, y_train_sin_reg), _ = iai.split_data('regression', X_train2, np.array(y_train_sin), train_proportion=0.9999)\n",
    "    ###BASELINES\n",
    "    y_test_baseline_speed, y_test_baseline_cos_wind, y_test_baseline_sin_wind, y_test_baseline_dangerous_scenarios, y_test_baseline_scenarios = get_baselines(\n",
    "        x_df, x)\n",
    "    y_train_baseline_speed, y_train_baseline_cos_wind, y_train_baseline_sin_wind, y_train_baseline_dangerous_scenarios, y_train_baseline_scenarios = get_baselines_train(\n",
    "        x_df, x)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_cos = '/home/gridsan/leobix/safi/Trees3/vfinal_Regression_tree_cos_in48_out3_sparsity_0.json'\n",
    "path_sin = '/home/gridsan/leobix/safi/Trees3/vfinal_Regression_tree_sin_in48_out3_sparsity_0.json'\n",
    "path_speed = '/home/gridsan/leobix/safi/Trees3/vfinal_Regression_tree_speed_in48_out3_sparsity_0.json'\n",
    "path_class = '/home/gridsan/leobix/safi/Trees/vfinal_Classification_tree_dangerous_in48_out3_criterion_gini.json'\n",
    "\n",
    "lnr_cos = iai.read_json(path_cos)\n",
    "lnr_sin = iai.read_json(path_sin)\n",
    "lnr_speed = iai.read_json(path_speed)\n",
    "lnr_dangerous = iai.read_json(path_class)\n",
    "\n",
    "y_hat_speed = lnr_speed.predict(X_test)\n",
    "y_hat_cos = lnr_cos.predict(X_test)\n",
    "y_hat_sin = lnr_sin.predict(X_test)\n",
    "y_hat_dangerous_from_regression = get_all_dangerous_scenarios(y_hat_speed, y_hat_cos, y_hat_sin)\n",
    "y_hat_dangerous_from_classification_proba = np.array(lnr_dangerous.predict_proba(X_test)['1'])\n",
    "y_hat_dangerous_from_classification = np.array(lnr_dangerous.predict(X_test))\n",
    "\n",
    "y_hat_speed_train = lnr_speed.predict(X_train)\n",
    "y_hat_cos_train = lnr_cos.predict(X_train)\n",
    "y_hat_sin_train = lnr_sin.predict(X_train)\n",
    "y_hat_dangerous_from_regression_train = get_all_dangerous_scenarios(y_hat_speed_train, y_hat_cos_train, y_hat_sin_train)\n",
    "y_hat_dangerous_from_classification_proba_train = np.array(lnr_dangerous.predict_proba(X_train)['1'])\n",
    "y_hat_dangerous_from_classification_train = np.array(lnr_dangerous.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ensemble = pd.read_csv('results_ensemble/xgboost_result_train_3.csv?dl=0')\n",
    "X_test_ensemble = pd.read_csv('results_ensemble/xgboost_result_test_3.csv?dl=0')\n",
    "X_train_ensemble = X_train_ensemble.drop(['true', 'baseline'], axis = 1)\n",
    "#X_train_ensemble = X_train_ensemble.drop([0], axis = 0)\n",
    "X_test_ensemble = X_test_ensemble.drop(['true', 'baseline'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ensemble['OCT_speed'] = y_hat_speed\n",
    "X_test_ensemble['OCT_cos'] = y_hat_cos\n",
    "X_test_ensemble['OCT_sin'] = y_hat_sin\n",
    "X_test_ensemble['OCT_indirect'] = y_hat_dangerous_from_regression\n",
    "X_test_ensemble['OCT_direct'] = y_hat_dangerous_from_classification\n",
    "X_test_ensemble['OCT_direct_probas'] = y_hat_dangerous_from_classification_proba\n",
    "X_test_ensemble['NUMTECH_speed'] = y_test_baseline_speed\n",
    "X_test_ensemble['NUMTECH_cos'] = y_test_baseline_cos_wind\n",
    "X_test_ensemble['NUMTECH_sin'] = y_test_baseline_sin_wind\n",
    "X_test_ensemble['NUMTECH_indirect'] = y_test_baseline_dangerous_scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ensemble['OCT_speed'] = y_hat_speed_train\n",
    "X_train_ensemble['OCT_cos'] = y_hat_cos_train\n",
    "X_train_ensemble['OCT_sin'] = y_hat_sin_train\n",
    "X_train_ensemble['OCT_indirect'] = y_hat_dangerous_from_regression_train\n",
    "X_train_ensemble['OCT_direct'] = y_hat_dangerous_from_classification_train\n",
    "X_train_ensemble['OCT_direct_probas'] = y_hat_dangerous_from_classification_proba_train\n",
    "X_train_ensemble['NUMTECH_speed'] = y_train_baseline_speed\n",
    "X_train_ensemble['NUMTECH_cos'] = y_train_baseline_cos_wind\n",
    "X_train_ensemble['NUMTECH_sin'] = y_train_baseline_sin_wind\n",
    "X_train_ensemble['NUMTECH_indirect'] = y_train_baseline_dangerous_scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### Costs are ######\n",
      "NUMTECH:  1067\n",
      "OCT indirect:  691\n",
      "OCT direct:  848\n",
      "XGB indirect:  591\n",
      "XGB direct:  847\n",
      "Majority vote  582\n"
     ]
    }
   ],
   "source": [
    "c = confusion_matrix(y_test_dangerous,X_test_ensemble['NUMTECH_indirect'])\n",
    "#print(f1_score(y_test_dangerous, consensus_majority, average = 'macro'))\n",
    "print('###### Costs are ######')\n",
    "print('NUMTECH: ', c[0,1]*2+c[1,0]*3)\n",
    "\n",
    "c = confusion_matrix(y_test_dangerous,X_test_ensemble['OCT_indirect'])\n",
    "print('OCT indirect: ', c[0,1]*2+c[1,0]*3)\n",
    "\n",
    "c = confusion_matrix(y_test_dangerous,X_test_ensemble['OCT_direct'])\n",
    "print('OCT direct: ', c[0,1]*2+c[1,0]*3)\n",
    "\n",
    "c = confusion_matrix(y_test_dangerous,X_test_ensemble['dangerous_indirect'])\n",
    "print('XGB indirect: ', c[0,1]*2+c[1,0]*3)\n",
    "\n",
    "c = confusion_matrix(y_test_dangerous,X_test_ensemble['dangerous'])\n",
    "print('XGB direct: ', c[0,1]*2+c[1,0]*3)\n",
    "\n",
    "X_test_majority_vote = X_test_ensemble[['NUMTECH_indirect', 'OCT_direct', 'dangerous_indirect', 'dangerous_indirect', 'dangerous']]\n",
    "consensus_majority = np.array(X_test_majority_vote.sum(axis = 1) >=3)*1\n",
    "c = confusion_matrix(y_test_dangerous,consensus_majority)\n",
    "print('Majority vote ', c[0,1]*2+c[1,0]*3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 0.3, 'penalty': 'l1'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.951 (+/-0.062) for {'C': 1, 'penalty': 'l2'}\n",
      "0.950 (+/-0.055) for {'C': 1, 'penalty': 'l1'}\n",
      "0.952 (+/-0.059) for {'C': 2, 'penalty': 'l2'}\n",
      "0.949 (+/-0.052) for {'C': 2, 'penalty': 'l1'}\n",
      "0.940 (+/-0.074) for {'C': 0.1, 'penalty': 'l2'}\n",
      "0.951 (+/-0.055) for {'C': 0.1, 'penalty': 'l1'}\n",
      "0.938 (+/-0.075) for {'C': 0.05, 'penalty': 'l2'}\n",
      "0.948 (+/-0.071) for {'C': 0.05, 'penalty': 'l1'}\n",
      "0.943 (+/-0.070) for {'C': 0.2, 'penalty': 'l2'}\n",
      "0.951 (+/-0.056) for {'C': 0.2, 'penalty': 'l1'}\n",
      "0.946 (+/-0.068) for {'C': 0.3, 'penalty': 'l2'}\n",
      "0.953 (+/-0.056) for {'C': 0.3, 'penalty': 'l1'}\n",
      "0.949 (+/-0.055) for {'C': 10, 'penalty': 'l2'}\n",
      "0.948 (+/-0.052) for {'C': 10, 'penalty': 'l1'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98      8276\n",
      "           1       0.33      0.52      0.40       218\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      8494\n",
      "   macro avg       0.66      0.75      0.69      8494\n",
      "weighted avg       0.97      0.96      0.96      8494\n",
      "\n",
      "Ensemble  784\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 1, 'penalty': 'l1'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.930 (+/-0.058) for {'C': 1, 'penalty': 'l2'}\n",
      "0.934 (+/-0.056) for {'C': 1, 'penalty': 'l1'}\n",
      "0.932 (+/-0.057) for {'C': 2, 'penalty': 'l2'}\n",
      "0.934 (+/-0.059) for {'C': 2, 'penalty': 'l1'}\n",
      "0.914 (+/-0.060) for {'C': 0.1, 'penalty': 'l2'}\n",
      "0.928 (+/-0.051) for {'C': 0.1, 'penalty': 'l1'}\n",
      "0.910 (+/-0.062) for {'C': 0.05, 'penalty': 'l2'}\n",
      "0.911 (+/-0.064) for {'C': 0.05, 'penalty': 'l1'}\n",
      "0.919 (+/-0.063) for {'C': 0.2, 'penalty': 'l2'}\n",
      "0.931 (+/-0.053) for {'C': 0.2, 'penalty': 'l1'}\n",
      "0.922 (+/-0.065) for {'C': 0.3, 'penalty': 'l2'}\n",
      "0.933 (+/-0.051) for {'C': 0.3, 'penalty': 'l1'}\n",
      "0.933 (+/-0.061) for {'C': 10, 'penalty': 'l2'}\n",
      "0.933 (+/-0.060) for {'C': 10, 'penalty': 'l1'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98      8276\n",
      "           1       0.32      0.52      0.40       218\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      8494\n",
      "   macro avg       0.66      0.75      0.69      8494\n",
      "weighted avg       0.97      0.96      0.96      8494\n",
      "\n",
      "Ensemble  786\n"
     ]
    }
   ],
   "source": [
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'penalty': ['l2', 'l1'], 'C': [1, 2, 0.1, 0.05, 0.2, 0.3, 10]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "        LogisticRegression(), tuned_parameters, scoring='%s_macro' % score\n",
    "    )\n",
    "    clf.fit(X_train_ensemble, y_train_dangerous)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test_dangerous, clf.predict(X_test_ensemble)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    c = confusion_matrix(y_test_dangerous,y_pred)\n",
    "    print('Ensemble ', c[0,1]*2+c[1,0]*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'learning_rate': 0.3, 'max_depth': 2}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.965 (+/-0.044) for {'learning_rate': 0.3, 'max_depth': 2}\n",
      "0.961 (+/-0.041) for {'learning_rate': 0.3, 'max_depth': 3}\n",
      "0.959 (+/-0.047) for {'learning_rate': 0.3, 'max_depth': 4}\n",
      "0.958 (+/-0.050) for {'learning_rate': 0.3, 'max_depth': 5}\n",
      "0.964 (+/-0.052) for {'learning_rate': 0.2, 'max_depth': 2}\n",
      "0.963 (+/-0.045) for {'learning_rate': 0.2, 'max_depth': 3}\n",
      "0.963 (+/-0.046) for {'learning_rate': 0.2, 'max_depth': 4}\n",
      "0.960 (+/-0.047) for {'learning_rate': 0.2, 'max_depth': 5}\n",
      "0.964 (+/-0.054) for {'learning_rate': 0.15, 'max_depth': 2}\n",
      "0.964 (+/-0.047) for {'learning_rate': 0.15, 'max_depth': 3}\n",
      "0.964 (+/-0.046) for {'learning_rate': 0.15, 'max_depth': 4}\n",
      "0.962 (+/-0.051) for {'learning_rate': 0.15, 'max_depth': 5}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98      8276\n",
      "           1       0.32      0.47      0.38       218\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      8494\n",
      "   macro avg       0.65      0.72      0.68      8494\n",
      "weighted avg       0.97      0.96      0.96      8494\n",
      "\n",
      "Ensemble  777\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'learning_rate': 0.15, 'max_depth': 4}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.945 (+/-0.046) for {'learning_rate': 0.3, 'max_depth': 2}\n",
      "0.946 (+/-0.045) for {'learning_rate': 0.3, 'max_depth': 3}\n",
      "0.947 (+/-0.038) for {'learning_rate': 0.3, 'max_depth': 4}\n",
      "0.946 (+/-0.038) for {'learning_rate': 0.3, 'max_depth': 5}\n",
      "0.944 (+/-0.047) for {'learning_rate': 0.2, 'max_depth': 2}\n",
      "0.945 (+/-0.041) for {'learning_rate': 0.2, 'max_depth': 3}\n",
      "0.947 (+/-0.041) for {'learning_rate': 0.2, 'max_depth': 4}\n",
      "0.949 (+/-0.039) for {'learning_rate': 0.2, 'max_depth': 5}\n",
      "0.944 (+/-0.047) for {'learning_rate': 0.15, 'max_depth': 2}\n",
      "0.945 (+/-0.044) for {'learning_rate': 0.15, 'max_depth': 3}\n",
      "0.950 (+/-0.040) for {'learning_rate': 0.15, 'max_depth': 4}\n",
      "0.947 (+/-0.037) for {'learning_rate': 0.15, 'max_depth': 5}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98      8276\n",
      "           1       0.32      0.46      0.38       218\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      8494\n",
      "   macro avg       0.65      0.72      0.68      8494\n",
      "weighted avg       0.97      0.96      0.96      8494\n",
      "\n",
      "Ensemble  778\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'max_depth': [2,3,4,5], 'learning_rate': [0.3, 0.2, 0.15]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "        XGBClassifier(tree_method='gpu_hist', gpu_id=0), tuned_parameters, scoring='%s_macro' % score\n",
    "    )\n",
    "    clf.fit(X_train_ensemble, y_train_dangerous)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    y_true, y_pred = y_test_dangerous, clf.predict(X_test_ensemble)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    c = confusion_matrix(y_test_dangerous,y_pred)\n",
    "    print('Ensemble ', c[0,1]*2+c[1,0]*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'learning_rate': 0.15, 'max_depth': 2}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.804 (+/-0.116) for {'learning_rate': 0.3, 'max_depth': 2}\n",
      "0.785 (+/-0.139) for {'learning_rate': 0.3, 'max_depth': 3}\n",
      "0.790 (+/-0.164) for {'learning_rate': 0.3, 'max_depth': 4}\n",
      "0.771 (+/-0.150) for {'learning_rate': 0.3, 'max_depth': 5}\n",
      "0.827 (+/-0.119) for {'learning_rate': 0.2, 'max_depth': 2}\n",
      "0.783 (+/-0.111) for {'learning_rate': 0.2, 'max_depth': 3}\n",
      "0.795 (+/-0.182) for {'learning_rate': 0.2, 'max_depth': 4}\n",
      "0.782 (+/-0.186) for {'learning_rate': 0.2, 'max_depth': 5}\n",
      "0.844 (+/-0.159) for {'learning_rate': 0.15, 'max_depth': 2}\n",
      "0.797 (+/-0.125) for {'learning_rate': 0.15, 'max_depth': 3}\n",
      "0.795 (+/-0.131) for {'learning_rate': 0.15, 'max_depth': 4}\n",
      "0.789 (+/-0.171) for {'learning_rate': 0.15, 'max_depth': 5}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      8276\n",
      "           1       0.83      0.44      0.58       218\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      8494\n",
      "   macro avg       0.91      0.72      0.79      8494\n",
      "weighted avg       0.98      0.98      0.98      8494\n",
      "\n",
      "Ensemble  403\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'learning_rate': 0.2, 'max_depth': 5}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.661 (+/-0.123) for {'learning_rate': 0.3, 'max_depth': 2}\n",
      "0.658 (+/-0.143) for {'learning_rate': 0.3, 'max_depth': 3}\n",
      "0.667 (+/-0.131) for {'learning_rate': 0.3, 'max_depth': 4}\n",
      "0.659 (+/-0.135) for {'learning_rate': 0.3, 'max_depth': 5}\n",
      "0.655 (+/-0.123) for {'learning_rate': 0.2, 'max_depth': 2}\n",
      "0.658 (+/-0.144) for {'learning_rate': 0.2, 'max_depth': 3}\n",
      "0.660 (+/-0.136) for {'learning_rate': 0.2, 'max_depth': 4}\n",
      "0.671 (+/-0.124) for {'learning_rate': 0.2, 'max_depth': 5}\n",
      "0.655 (+/-0.128) for {'learning_rate': 0.15, 'max_depth': 2}\n",
      "0.650 (+/-0.133) for {'learning_rate': 0.15, 'max_depth': 3}\n",
      "0.656 (+/-0.133) for {'learning_rate': 0.15, 'max_depth': 4}\n",
      "0.653 (+/-0.127) for {'learning_rate': 0.15, 'max_depth': 5}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      8276\n",
      "           1       1.00      0.97      0.98       218\n",
      "\n",
      "   micro avg       1.00      1.00      1.00      8494\n",
      "   macro avg       1.00      0.98      0.99      8494\n",
      "weighted avg       1.00      1.00      1.00      8494\n",
      "\n",
      "Ensemble  21\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'max_depth': [2,3,4,5], 'learning_rate': [0.3, 0.2, 0.15]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "        XGBClassifier(tree_method='gpu_hist', gpu_id=0), tuned_parameters, scoring='%s_macro' % score\n",
    "    )\n",
    "    clf.fit(X_test_ensemble, y_test_dangerous)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    y_true, y_pred = y_test_dangerous, clf.predict(X_test_ensemble)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    c = confusion_matrix(y_test_dangerous,y_pred)\n",
    "    print('Ensemble ', c[0,1]*2+c[1,0]*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'alpha': 0.001, 'hidden_layer_sizes': (16, 8)}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.955 (+/-0.034) for {'alpha': 0.001, 'hidden_layer_sizes': (16, 8)}\n",
      "0.947 (+/-0.075) for {'alpha': 0.001, 'hidden_layer_sizes': 16}\n",
      "0.949 (+/-0.049) for {'alpha': 0.001, 'hidden_layer_sizes': 32}\n",
      "0.945 (+/-0.056) for {'alpha': 0.001, 'hidden_layer_sizes': 8}\n",
      "0.949 (+/-0.047) for {'alpha': 0.001, 'hidden_layer_sizes': (32, 8)}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98      8276\n",
      "           1       0.30      0.50      0.38       218\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      8494\n",
      "   macro avg       0.64      0.74      0.68      8494\n",
      "weighted avg       0.97      0.96      0.96      8494\n",
      "\n",
      "Ensemble  830\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'alpha': 0.001, 'hidden_layer_sizes': (16, 8)}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.951 (+/-0.028) for {'alpha': 0.001, 'hidden_layer_sizes': (16, 8)}\n",
      "0.943 (+/-0.055) for {'alpha': 0.001, 'hidden_layer_sizes': 16}\n",
      "0.944 (+/-0.054) for {'alpha': 0.001, 'hidden_layer_sizes': 32}\n",
      "0.932 (+/-0.059) for {'alpha': 0.001, 'hidden_layer_sizes': 8}\n",
      "0.942 (+/-0.065) for {'alpha': 0.001, 'hidden_layer_sizes': (32, 8)}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98      8276\n",
      "           1       0.33      0.50      0.40       218\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      8494\n",
      "   macro avg       0.66      0.73      0.69      8494\n",
      "weighted avg       0.97      0.96      0.96      8494\n",
      "\n",
      "Ensemble  770\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'hidden_layer_sizes': [(16, 8),(16),(32),(8),(32,8)], 'alpha': [0.001]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "        MLPClassifier(), tuned_parameters, scoring='%s_macro' % score\n",
    "    )\n",
    "    clf.fit(X_train_ensemble, y_train_dangerous)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    y_true, y_pred = y_test_dangerous, clf.predict(X_test_ensemble)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    c = confusion_matrix(y_test_dangerous,y_pred)\n",
    "    print('Ensemble ', c[0,1]*2+c[1,0]*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lnr_dangerous.score(X_test, y_test_dangerous, criterion='misclassification'))\n",
    "preds = lnr_dangerous.predict(X_test)\n",
    "print(confusion_matrix(y_test_dangerous,preds))\n",
    "print(f1_score(y_test_dangerous, preds, average = 'macro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
